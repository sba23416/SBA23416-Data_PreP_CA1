{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "892e7d6c",
   "metadata": {},
   "source": [
    "# Hodan Mohamed Abdi SBA23416_Data Preperation_CA1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1fe033",
   "metadata": {},
   "source": [
    "# Characterisation of the data set- aps_failure_set (1).csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f7a36d",
   "metadata": {},
   "source": [
    "<font color=\"blue\"> Introduction: \n",
    "\n",
    "My assignment i have been asked by Haulage company to analyse a dataset based on data collected from heavy Scania trucks in everyday usage. The system in focus is the Air Pressure system (APS) which generates pressurised air that are utilized in various functions in a truck, such as braking and gear changes.\n",
    "\n",
    "*The dataset’s <font color=\"green\"> positive class consists of component failures for a specific component of the APS system. text</font> \n",
    "\n",
    "*The <font color=\"red\"> negative class consists of trucks with failures for components not related to the APS\n",
    "\n",
    "*The data consists of a subset of all available data, selected by experts.\n",
    "\n",
    "*The main aim of this analysis will help determine the investment strategy for the company in the upcoming year.\n",
    "\n",
    "All data wrangling, analysis, and visualizations must be generated using python.\n",
    "\n",
    "The companies CTO also requires that i must include rationalize all the decisions that you have made in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b173cc6",
   "metadata": {},
   "source": [
    "Assignment Minimum Requirements: \n",
    "\n",
    "Step 1 of the assignment is to: use the dataset contained within the file “aps_failure_set.csv”, conduct the following analysis and report with my findings as following step below:\n",
    "\n",
    "1.Characterisation of the data set: Which incldues the size; How many number of attributes; if there are or has/does not have missing values, Also listing the number of observations. and idenfying what these Characterisation mean.\n",
    "\n",
    "I will be importing the dataset with Panda library to verify what are the specific's\n",
    "\n",
    "Step 2:Application of Data preparation & Evaluation Methods: Include the following:Cleaning, renaming, & Exploratory Data Analysis (EDA) to get a better understanding of the data-set by summarizing its main characteristics and often plotting them visually.\n",
    "\n",
    "I have come to understand that this step is very important especially when we arrive at modelling the data to apply Machine learning. Plotting in EDA consists of Histograms, Box plot, Scatter plots.\n",
    "\n",
    "Step 3: Use Principal Component Analysis, In this section, i will be explore what is perhaps one of the most broadly used of unsupervised algorithms, principal component analysis (PCA). PCA is fundamentally a dimensionality reduction algorithm, but it can also be useful as a tool for visualization, for noise filtering,\n",
    "\n",
    "Step 4:Final Part: Curse of Dimensionality, I will end the assignment with my finding toward my analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25770cc1",
   "metadata": {},
   "source": [
    "# 1. Characterisation of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9be815ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "sns.set() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75f3272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df =pd.read_csv (\"aps_failure_set (2).csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f555d84",
   "metadata": {},
   "source": [
    "<font color=\"green\"> Size of the dataset text</font> : In this section i have identified the size of the dataset using the follow code below: Dataset size: (6000 , 171)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f7e5d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size: (60000, 171)\n"
     ]
    }
   ],
   "source": [
    "size = aps_failure_df.shape\n",
    "print(\"Dataset Size:\", size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb143ca",
   "metadata": {},
   "source": [
    "<font color=\"green\"> Data Information text</font>: In this section, I have highlighted the current datafrane which includes the following, range index, columns, dtypes and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81912918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Columns: 171 entries, class to eg_000\n",
      "dtypes: int64(1), object(170)\n",
      "memory usage: 78.3+ MB\n"
     ]
    }
   ],
   "source": [
    "aps_failure_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193f353b",
   "metadata": {},
   "source": [
    "To get a better understanding of the dataset, I would need to identify rhe Row's and Columns and displaying the dtype value counts. as you can cledarly see within the below that the has 6000 Rows and 171 Colums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f98e9e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data set has 60000 rows and 171 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa_000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.000000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.933650e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.454301e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.340000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.077600e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.866800e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.746564e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             aa_000\n",
       "count  6.000000e+04\n",
       "mean   5.933650e+04\n",
       "std    1.454301e+05\n",
       "min    0.000000e+00\n",
       "25%    8.340000e+02\n",
       "50%    3.077600e+04\n",
       "75%    4.866800e+04\n",
       "max    2.746564e+06"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>aa_000</th>\n",
       "      <th>ab_000</th>\n",
       "      <th>ac_000</th>\n",
       "      <th>ad_000</th>\n",
       "      <th>ae_000</th>\n",
       "      <th>af_000</th>\n",
       "      <th>ag_000</th>\n",
       "      <th>ag_001</th>\n",
       "      <th>ag_002</th>\n",
       "      <th>...</th>\n",
       "      <th>ee_002</th>\n",
       "      <th>ee_003</th>\n",
       "      <th>ee_004</th>\n",
       "      <th>ee_005</th>\n",
       "      <th>ee_006</th>\n",
       "      <th>ee_007</th>\n",
       "      <th>ee_008</th>\n",
       "      <th>ee_009</th>\n",
       "      <th>ef_000</th>\n",
       "      <th>eg_000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>76698</td>\n",
       "      <td>na</td>\n",
       "      <td>2130706438</td>\n",
       "      <td>280</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1240520</td>\n",
       "      <td>493384</td>\n",
       "      <td>721044</td>\n",
       "      <td>469792</td>\n",
       "      <td>339156</td>\n",
       "      <td>157956</td>\n",
       "      <td>73224</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>33058</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>421400</td>\n",
       "      <td>178064</td>\n",
       "      <td>293306</td>\n",
       "      <td>245416</td>\n",
       "      <td>133654</td>\n",
       "      <td>81140</td>\n",
       "      <td>97576</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neg</td>\n",
       "      <td>41040</td>\n",
       "      <td>na</td>\n",
       "      <td>228</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>277378</td>\n",
       "      <td>159812</td>\n",
       "      <td>423992</td>\n",
       "      <td>409564</td>\n",
       "      <td>320746</td>\n",
       "      <td>158022</td>\n",
       "      <td>95128</td>\n",
       "      <td>514</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>240</td>\n",
       "      <td>46</td>\n",
       "      <td>58</td>\n",
       "      <td>44</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>60874</td>\n",
       "      <td>na</td>\n",
       "      <td>1368</td>\n",
       "      <td>458</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>622012</td>\n",
       "      <td>229790</td>\n",
       "      <td>405298</td>\n",
       "      <td>347188</td>\n",
       "      <td>286954</td>\n",
       "      <td>311560</td>\n",
       "      <td>433954</td>\n",
       "      <td>1218</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 171 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  class  aa_000 ab_000      ac_000 ad_000 ae_000 af_000 ag_000 ag_001 ag_002  \\\n",
       "0   neg   76698     na  2130706438    280      0      0      0      0      0   \n",
       "1   neg   33058     na           0     na      0      0      0      0      0   \n",
       "2   neg   41040     na         228    100      0      0      0      0      0   \n",
       "3   neg      12      0          70     66      0     10      0      0      0   \n",
       "4   neg   60874     na        1368    458      0      0      0      0      0   \n",
       "\n",
       "   ...   ee_002  ee_003  ee_004  ee_005  ee_006  ee_007  ee_008 ee_009 ef_000  \\\n",
       "0  ...  1240520  493384  721044  469792  339156  157956   73224      0      0   \n",
       "1  ...   421400  178064  293306  245416  133654   81140   97576   1500      0   \n",
       "2  ...   277378  159812  423992  409564  320746  158022   95128    514      0   \n",
       "3  ...      240      46      58      44      10       0       0      0      4   \n",
       "4  ...   622012  229790  405298  347188  286954  311560  433954   1218      0   \n",
       "\n",
       "  eg_000  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3     32  \n",
       "4      0  \n",
       "\n",
       "[5 rows x 171 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object    170\n",
      "int64       1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"The data set has {} rows and {} columns\".format(aps_failure_df.shape[0], aps_failure_df.shape[1]))\n",
    "display(aps_failure_df.describe())\n",
    "display(aps_failure_df.head())\n",
    "print(aps_failure_df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9a28b1",
   "metadata": {},
   "source": [
    "I have Identified the following: Object = 171, Int64 = 1 & dtype = int64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebb4ab0",
   "metadata": {},
   "source": [
    "# Identfying Atributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab4ea1c",
   "metadata": {},
   "source": [
    "In summary, identifying attributes and observations is a critical step in data analysis and modeling. It forms the basis for understanding the dataset's structure, selecting relevant features, and ensuring data quality. This information enables informed decisions, helps with feature engineering, and ultimately leads to more accurate and effective data analysis and modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8e124bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Attributes: 171\n"
     ]
    }
   ],
   "source": [
    "num_attributes = len(aps_failure_df.columns)\n",
    "print(\"Number of Attributes:\", num_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecac661f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Observations: 60000\n"
     ]
    }
   ],
   "source": [
    "num_observations = aps_failure_df.shape[0]\n",
    "print(\"Number of Observations:\", num_observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dd515a",
   "metadata": {},
   "source": [
    "# Identfying Dublicate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b933104a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aps_failure_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc29ab66",
   "metadata": {},
   "source": [
    "Zero Dublication were found in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a512d26f",
   "metadata": {},
   "source": [
    "# Identyfing Missing data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa17028",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b20f6b",
   "metadata": {},
   "source": [
    "# Idenfying what these Characterisation mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5519a232",
   "metadata": {},
   "source": [
    "The dataset i'm currently analysing consist of 600 rows (Observatios) & 171 Colums (Attributes), This makes it a modertely large dataset. Among these attributes, there are 171 distinc ones.\n",
    "\n",
    "In my analysis, I'm taking steps to identify the duplicated data entries, which helps maintain the datasets intergrity. Addtionally, I will be addrressing the missing values  (Na, NAN) to ensure that any fields with non-numeric or null vales are appropriatley transformed into numerica data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f4f7b1",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018538fe",
   "metadata": {},
   "source": [
    "Exploratory data analysis I have found to be a curitical step in the data analysis process for serval important reasons are as understanding the dataset, this will allow me to explore thestructure, conects and charartics of the data. EDA process will hopefully allow me to identfy the patterns and relationships within my dataset, this will also allow myself to spot trends, anomalies and dependencies between variabl that might not be apparent at fist. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f27e881",
   "metadata": {},
   "source": [
    "Data Quality Assurance, will help me identify and address data quality issues such as missing values, outliers and inconsistencies. By addressing this early will enable myself to ensure the anaylsis is clean and reliable. Below as you can see I have deployed aps_failure_df.describe(include =\"object\") to identified the NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6a6aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df.describe(include =\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa7dc64",
   "metadata": {},
   "source": [
    "As displayed above dataset, I have identified that there are dataset with NA in the column of ab_000, ad_000 which i have conclued that this would require to bechanged, although i still need to analys more of my dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c741421c",
   "metadata": {},
   "source": [
    "# Application of Data preparation & Evaluation methods "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdaa187",
   "metadata": {},
   "source": [
    "# Cleaning Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f787eb6d",
   "metadata": {},
   "source": [
    "This part of the process I will be working on identifying the unqiue value as well as the isnull values. As you can see that aps.failure_df has shown that there are quit a few datasets that are unique and now we can move to identifying isnull dataset to unsure that am working with consistant datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff582db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df[\"ab_000\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85362db",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df[\"ab_000\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bc42c7",
   "metadata": {},
   "source": [
    "As you can see above that there is 0 isnull with sum. am happy to continue with the cleaning of my dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964d466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df[\"ad_000\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f38c2e",
   "metadata": {},
   "source": [
    "In order to define the unique values within my dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cf78d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df[\"ad_000\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e61cd31",
   "metadata": {},
   "source": [
    "Next Step is to identfy the missing value using the missing_value as this will be valuable for the dataset analysis preprocessing. It is important to identfy the missing values in each columns (Attributes) within my dataset. The reason for this process as i would agree that its very much criticial as it helps me understand the qualitgy of dataset and also highlighting key area's that require cleaning or renaming dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eba146",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values=[\"na\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3adc101",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df=pd.read_csv(\"aps_failure_set (2).csv\" , na_values=missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ced077",
   "metadata": {},
   "source": [
    "My next step to chaning the missing values to number will be as followed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5a6f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value=aps_failure_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45a0a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(missing_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e8b700",
   "metadata": {},
   "source": [
    "As you can see that the missing valued has now been replaced with unique values with the application of using EDA method, Although that been said, With the remaining columns that are no longer required can be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3004b71",
   "metadata": {},
   "source": [
    "# Removing - Droping Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400f71ff",
   "metadata": {},
   "source": [
    "Steps with removing the columns are as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aa07e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "aps_failure_df = aps_failure_df.replace('na', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0055bebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df=aps_failure_df.drop(aps_failure_df.columns[aps_failure_df.isnull().mean()>10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67051756",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df.fillna(aps_failure_df.median(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a101b2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eabeb24",
   "metadata": {},
   "source": [
    "# Clearned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6ce665",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af66d091",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b4e14c",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis or (EDA) is understanding the data set by summarizing its main characteristics and often plotting them visually. This step is very important especially when we arrive at modelling the data to apply Machine learning. Plotting in EDA consists of Histograms, Box plot, Scatter plots and many more.\n",
    "\n",
    "Through the process of EDA, we can also refine the problem statement or definition of our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c23792e",
   "metadata": {},
   "source": [
    "# Boxplot Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289bc592",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=aps_failure_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e6123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=aps_failure_df[\"aa_000\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb04bae",
   "metadata": {},
   "source": [
    "# Historgrams for Dataset Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c90dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df = pd.read_csv(\"aps_failure_set (2).csv\")\n",
    "aps_failure_df.hist(bins=40, figsize=(15, 10))\n",
    "plt.suptitle(\"Histograms for Dataset Attributes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbd9a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea90a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count = aps_failure_df[\"class\"].value_counts()\n",
    "print(class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cbd2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = pd.Series([5900, 1000], index=['Negative', 'Positive'])\n",
    "plt.figure(figsize=(6, 4))\n",
    "class_counts.plot(kind= 'bar', color=['Blue', 'purple'])\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Units')\n",
    "plt.title('Air Pressur system-Apps Failure')\n",
    "plt.xticks(range(len(class_count.index)), ['Negative', 'Positive'], rotation=0)\n",
    "plt.legend(['The Nagative is not Related to Air Pressure system'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7429dce",
   "metadata": {},
   "source": [
    "What data are we exploring today?\n",
    "\n",
    "We are going to look at a data set on cars called “cardata.csv”.\n",
    "\n",
    "The data contains more than 6,000 rows and more than 171 columns which have features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617e9a2b",
   "metadata": {},
   "source": [
    "Assessing a Specific col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d65a24",
   "metadata": {},
   "source": [
    "Looking at Numerical data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af297339",
   "metadata": {},
   "source": [
    "# Step 4: Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47007327",
   "metadata": {},
   "source": [
    " Principal Component Analysis, In this section, i will be explore what is perhaps one of the most broadly used of unsupervised algorithms, principal component analysis (PCA). PCA is fundamentally a dimensionality reduction algorithm, but it can also be useful as a tool for visualization, for noise filtering,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617b72c7",
   "metadata": {},
   "source": [
    "# Retain 99.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99222c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "L_encoder = LabelEncoder()\n",
    "aps_failure_df[\"class\"]=L_encoder.fit_transform(aps_failure_df[\"class\"])\n",
    "aps_failure_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b3a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b9b104",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df_no_label = aps_failure_df.drop(aps_failure_df[\"class\"])\n",
    "pca = PCA().fit(aps_failure_df_no_label) \n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_)) \n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.axhline(0.9950,color= \"orangered\",alpha=.5,ls=\"--\")\n",
    "plt.axvline(2,color= \"orangered\",alpha=.5,ls=\"--\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfaf9ae",
   "metadata": {},
   "source": [
    "The main aim is to retain 99.5% variance data from the graphic show above, my understanding is that i must keep my features no higher or below 2 in order to get the results i am looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860d5842",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(10)\n",
    "projected = pca.fit_transform(failure_no_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6554d399",
   "metadata": {},
   "outputs": [],
   "source": [
    "projected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204ee830",
   "metadata": {},
   "source": [
    "Side notes: observatins equalling to 59998 and having 10 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12694df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "projected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3cb5e3",
   "metadata": {},
   "source": [
    "The Array's format clearly displays that each of my obsersavtion has number of values of 10, although not the orginal values were present in the orginal dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5809a2f1",
   "metadata": {},
   "source": [
    "in order to understand the dataframe better and have a clear picture i have clearly displayed below: for the purpose of demonstrations, I have labled the colums all 10 of them below for visual purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da34fd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_pca = pd.DataFrame(projected, columns =['C1','C2','C3','C4','C5','C6','C7','C8','C9','C10',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebd1b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82324062",
   "metadata": {},
   "source": [
    "As you can see that all my columns are labled with C1 to C10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_pca[\"class\"] = aps_failure_df[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36749d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348d62da",
   "metadata": {},
   "source": [
    "Data set I want to work with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f710b72",
   "metadata": {},
   "source": [
    "As you can see that I will be selecting only the Frist Colum from C1 to C10. just before it get to the last column class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d657d2",
   "metadata": {},
   "source": [
    "PCA was used to establish the minimum number of features needed for retaining 99.5% variance in the data and then implement PCA to dimensionally reduce the data to the number of features that you have discovered. \n",
    "\n",
    "This will Include a clear and concise explanation of what you i will be doing with the data \n",
    "\n",
    "and why you are doing it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b024aaa",
   "metadata": {},
   "source": [
    "# Step 5:Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa7bac7",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28294bea",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae13f5f",
   "metadata": {},
   "source": [
    "Pratusevich, M. (2017a). Practice Python. [online] Practicepython.org. Available at: https://www.practicepython.org/.\n",
    "\n",
    "Vishal (2019). Python Basic Exercise for Beginners. [online] PYnative. Available at: https://pynative.com/python-basic-exercise-for-beginners/.\n",
    "\n",
    "John Paul Mueller and Luca Massaron (2023). Python for Data Science For Dummies. John Wiley & Sons.\n",
    "\n",
    "Idris, I. (2011). NumPy 1.5 : beginner’s guide. Birmingham: Packt Publishing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b8496a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
