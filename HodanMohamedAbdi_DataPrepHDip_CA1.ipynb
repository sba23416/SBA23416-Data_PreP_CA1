{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "892e7d6c",
   "metadata": {},
   "source": [
    "# Hodan Mohamed Abdi SBA23416_Data Preperation_CA1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1fe033",
   "metadata": {},
   "source": [
    "# Characterisation of the data set- aps_failure_set (1).csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f7a36d",
   "metadata": {},
   "source": [
    "<font color=\"blue\"> Introduction: \n",
    "\n",
    "My assignment i have been asked by Haulage company to analyse a dataset based on data collected from heavy Scania trucks in everyday usage. The system in focus is the Air Pressure system (APS) which generates pressurised air that are utilized in various functions in a truck, such as braking and gear changes.\n",
    "\n",
    "*The dataset’s <font color=\"green\"> positive class consists of component failures for a specific component of the APS system. text</font> \n",
    "\n",
    "*The <font color=\"red\"> negative class consists of trucks with failures for components not related to the APS\n",
    "\n",
    "*The data consists of a subset of all available data, selected by experts.\n",
    "\n",
    "*The main aim of this analysis will help determine the investment strategy for the company in the upcoming year.\n",
    "\n",
    "All data wrangling, analysis, and visualizations must be generated using python.\n",
    "\n",
    "The companies CTO also requires that i must include rationalize all the decisions that you have made in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b173cc6",
   "metadata": {},
   "source": [
    "Assignment Minimum Requirements: \n",
    "\n",
    "Step 1 of the assignment is to: use the dataset contained within the file “aps_failure_set.csv”, conduct the following analysis and report with my findings as following step below:\n",
    "\n",
    "1.Characterisation of the data set: Which incldues the size; How many number of attributes; if there are or has/does not have missing values, Also listing the number of observations. and idenfying what these Characterisation mean.\n",
    "\n",
    "I will be importing the dataset with Panda library to verify what are the specific's\n",
    "\n",
    "Step 2:Application of Data preparation & Evaluation Methods: Include the following:Cleaning, renaming, & Exploratory Data Analysis (EDA) to get a better understanding of the data-set by summarizing its main characteristics and often plotting them visually.\n",
    "\n",
    "I have come to understand that this step is very important especially when we arrive at modelling the data to apply Machine learning. Plotting in EDA consists of Histograms, Box plot, Scatter plots.\n",
    "\n",
    "Step 3: Use Principal Component Analysis, In this section, i will be explore what is perhaps one of the most broadly used of unsupervised algorithms, principal component analysis (PCA). PCA is fundamentally a dimensionality reduction algorithm, but it can also be useful as a tool for visualization, for noise filtering,\n",
    "\n",
    "Step 4:Final Part: Curse of Dimensionality, I will end the assignment with my finding toward my analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25770cc1",
   "metadata": {},
   "source": [
    "# 1. Characterisation of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9be815ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "sns.set() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75f3272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df =pd.read_csv (\"aps_failure_set (2).csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f555d84",
   "metadata": {},
   "source": [
    "<font color=\"green\"> Size of the dataset text</font> : In this section i have identified the size of the dataset using the follow code below: Dataset size: (6000 , 171)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f7e5d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size: (60000, 171)\n"
     ]
    }
   ],
   "source": [
    "size = aps_failure_df.shape\n",
    "print(\"Dataset Size:\", size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb143ca",
   "metadata": {},
   "source": [
    "<font color=\"green\"> Data Information text</font>: In this section, I have highlighted the current datafrane which includes the following, range index, columns, dtypes and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81912918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Columns: 171 entries, class to eg_000\n",
      "dtypes: int64(1), object(170)\n",
      "memory usage: 78.3+ MB\n"
     ]
    }
   ],
   "source": [
    "aps_failure_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193f353b",
   "metadata": {},
   "source": [
    "To get a better understanding of the dataset, I would need to identify rhe Row's and Columns and displaying the dtype value counts. as you can cledarly see within the below that the has 6000 Rows and 171 Colums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f98e9e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data set has 60000 rows and 171 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa_000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.000000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.933650e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.454301e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.340000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.077600e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.866800e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.746564e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             aa_000\n",
       "count  6.000000e+04\n",
       "mean   5.933650e+04\n",
       "std    1.454301e+05\n",
       "min    0.000000e+00\n",
       "25%    8.340000e+02\n",
       "50%    3.077600e+04\n",
       "75%    4.866800e+04\n",
       "max    2.746564e+06"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>aa_000</th>\n",
       "      <th>ab_000</th>\n",
       "      <th>ac_000</th>\n",
       "      <th>ad_000</th>\n",
       "      <th>ae_000</th>\n",
       "      <th>af_000</th>\n",
       "      <th>ag_000</th>\n",
       "      <th>ag_001</th>\n",
       "      <th>ag_002</th>\n",
       "      <th>...</th>\n",
       "      <th>ee_002</th>\n",
       "      <th>ee_003</th>\n",
       "      <th>ee_004</th>\n",
       "      <th>ee_005</th>\n",
       "      <th>ee_006</th>\n",
       "      <th>ee_007</th>\n",
       "      <th>ee_008</th>\n",
       "      <th>ee_009</th>\n",
       "      <th>ef_000</th>\n",
       "      <th>eg_000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>76698</td>\n",
       "      <td>na</td>\n",
       "      <td>2130706438</td>\n",
       "      <td>280</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1240520</td>\n",
       "      <td>493384</td>\n",
       "      <td>721044</td>\n",
       "      <td>469792</td>\n",
       "      <td>339156</td>\n",
       "      <td>157956</td>\n",
       "      <td>73224</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>33058</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>421400</td>\n",
       "      <td>178064</td>\n",
       "      <td>293306</td>\n",
       "      <td>245416</td>\n",
       "      <td>133654</td>\n",
       "      <td>81140</td>\n",
       "      <td>97576</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neg</td>\n",
       "      <td>41040</td>\n",
       "      <td>na</td>\n",
       "      <td>228</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>277378</td>\n",
       "      <td>159812</td>\n",
       "      <td>423992</td>\n",
       "      <td>409564</td>\n",
       "      <td>320746</td>\n",
       "      <td>158022</td>\n",
       "      <td>95128</td>\n",
       "      <td>514</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>240</td>\n",
       "      <td>46</td>\n",
       "      <td>58</td>\n",
       "      <td>44</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>60874</td>\n",
       "      <td>na</td>\n",
       "      <td>1368</td>\n",
       "      <td>458</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>622012</td>\n",
       "      <td>229790</td>\n",
       "      <td>405298</td>\n",
       "      <td>347188</td>\n",
       "      <td>286954</td>\n",
       "      <td>311560</td>\n",
       "      <td>433954</td>\n",
       "      <td>1218</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 171 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  class  aa_000 ab_000      ac_000 ad_000 ae_000 af_000 ag_000 ag_001 ag_002  \\\n",
       "0   neg   76698     na  2130706438    280      0      0      0      0      0   \n",
       "1   neg   33058     na           0     na      0      0      0      0      0   \n",
       "2   neg   41040     na         228    100      0      0      0      0      0   \n",
       "3   neg      12      0          70     66      0     10      0      0      0   \n",
       "4   neg   60874     na        1368    458      0      0      0      0      0   \n",
       "\n",
       "   ...   ee_002  ee_003  ee_004  ee_005  ee_006  ee_007  ee_008 ee_009 ef_000  \\\n",
       "0  ...  1240520  493384  721044  469792  339156  157956   73224      0      0   \n",
       "1  ...   421400  178064  293306  245416  133654   81140   97576   1500      0   \n",
       "2  ...   277378  159812  423992  409564  320746  158022   95128    514      0   \n",
       "3  ...      240      46      58      44      10       0       0      0      4   \n",
       "4  ...   622012  229790  405298  347188  286954  311560  433954   1218      0   \n",
       "\n",
       "  eg_000  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3     32  \n",
       "4      0  \n",
       "\n",
       "[5 rows x 171 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object    170\n",
      "int64       1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"The data set has {} rows and {} columns\".format(aps_failure_df.shape[0], aps_failure_df.shape[1]))\n",
    "display(aps_failure_df.describe())\n",
    "display(aps_failure_df.head())\n",
    "print(aps_failure_df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7e6219",
   "metadata": {},
   "source": [
    "I have Identified the following: Object = 171, Int64 = 1 & dtype = int64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebb4ab0",
   "metadata": {},
   "source": [
    "# Identfying Atributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab4ea1c",
   "metadata": {},
   "source": [
    "In summary, identifying attributes and observations is a critical step in data analysis and modeling. It forms the basis for understanding the dataset's structure, selecting relevant features, and ensuring data quality. This information enables informed decisions, helps with feature engineering, and ultimately leads to more accurate and effective data analysis and modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8e124bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Attributes: 171\n"
     ]
    }
   ],
   "source": [
    "num_attributes = len(aps_failure_df.columns)\n",
    "print(\"Number of Attributes:\", num_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecac661f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Observations: 60000\n"
     ]
    }
   ],
   "source": [
    "num_observations = aps_failure_df.shape[0]\n",
    "print(\"Number of Observations:\", num_observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dd515a",
   "metadata": {},
   "source": [
    "# Identfying Dublicate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b933104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc29ab66",
   "metadata": {},
   "source": [
    "Zero Dublication were found in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a512d26f",
   "metadata": {},
   "source": [
    "# Identyfing Missing data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa17028",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f504da32",
   "metadata": {},
   "source": [
    "•The class column contains the labels and its appears that the labels are being represented with the value of Zero’s\n",
    "\n",
    "•Lengh equals to 171\n",
    "\n",
    "•Data Types equales to int64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b20f6b",
   "metadata": {},
   "source": [
    "# Idenfying what these Characterisation mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5519a232",
   "metadata": {},
   "source": [
    "The dataset i'm currently analysing consist of 600 rows (Observatios) & 171 Colums (Attributes), This makes it a modertely large dataset. Among these attributes, there are 171 distinc ones.\n",
    "\n",
    "In my analysis, I'm taking steps to identify the duplicated data entries, which helps maintain the datasets intergrity. Addtionally, I will be addrressing the missing values  (Na, NAN) to ensure that any fields with non-numeric or null vales are appropriatley transformed into numerica data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f4f7b1",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc24c7f",
   "metadata": {},
   "source": [
    "Exploratory data analysis I have found to be a curitical step in the data analysis process for serval important reasons are as understanding the dataset, this will allow me to explore thestructure, conects and charartics of the data. EDA process will hopefully allow me to identfy the patterns and relationships within my dataset, this will also allow myself to spot trends, anomalies and dependencies between variabl that might not be apparent at fist. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab91e14",
   "metadata": {},
   "source": [
    "Data Quality Assurance, will help me identify and address data quality issues such as missing values, outliers and inconsistencies. By addressing this early will enable myself to ensure the anaylsis is clean and reliable. Below as you can see I have deployed aps_failure_df.describe(include =\"object\") to identified the NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6a6aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df.describe(include =\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa7dc64",
   "metadata": {},
   "source": [
    "As displayed above dataset, I have identified that there are dataset with NA in the column of ab_000, ad_000 which i have conclued that this would require to bechanged, although i still need to analys more of my dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826b258e",
   "metadata": {},
   "source": [
    "# Application of Data preparation & Evaluation methods "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f84598",
   "metadata": {},
   "source": [
    "# Cleaning Dataset Process "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2986116",
   "metadata": {},
   "source": [
    "This part of the process I will be working on identifying the unqiue value as well as the isnull values. As you can see that aps.failure_df has shown that there are quit a few datasets that are unique and now we can move to identifying isnull dataset to unsure that am working with consistant datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff582db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df[\"ab_000\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85362db",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df[\"ab_000\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e6b924",
   "metadata": {},
   "source": [
    "As you can see above that there is 0 isnull with sum. am happy to continue with the cleaning of my dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964d466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df[\"ad_000\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40122959",
   "metadata": {},
   "source": [
    "In order to define the unique values within my dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cf78d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df[\"ad_000\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275fe915",
   "metadata": {},
   "source": [
    "Next Step is to identfy the missing value using the missing_value as this will be valuable for the dataset analysis preprocessing. It is important to identfy the missing values in each columns (Attributes) within my dataset. The reason for this process as i would agree that its very much criticial as it helps me understand the qualitgy of dataset and also highlighting key area's that require cleaning or renaming dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eba146",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values=[\"NA\",\"N/A\", \"-\", \"NaN\", \"missing\", \"na\", \"?\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b1d10f",
   "metadata": {},
   "source": [
    "In order to identify and locate any missing value, i have decided to use the missing_value = ['na'] as part of my dataset cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3adc101",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df=pd.read_csv(\"aps_failure_set (2).csv\" , na_values=missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ced077",
   "metadata": {},
   "source": [
    "My next step to chaning the missing values to number will be as followed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5a6f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value=aps_failure_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f374b5c",
   "metadata": {},
   "source": [
    "In order to identify and quantify the missing vaules within APS_Failure_DF, I need this code to calculate the number of missing vailes for each column and store the results in the missing_value \"variables\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45a0a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(missing_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e8b700",
   "metadata": {},
   "source": [
    "As you can see that the missing valued has now been replaced with unique values with the application of using EDA method, Although that been said, With the remaining columns that are no longer required can be removed. \n",
    "by doing this i have gained the valuable insight into the data quality which will allow me to take informed action.\n",
    "\n",
    "It's key to understand which columns have missing data, especially how much data is missing in each column. this allows myself to take action that will be beneficial for my dataset and results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417383ac",
   "metadata": {},
   "source": [
    "# Removing - Droping Values - Changing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400f71ff",
   "metadata": {},
   "source": [
    "This Steps will disucss and highlight the process of removing the columns are as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be79e617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "aps_failure_df = aps_failure_df.replace('na', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91b1cb6",
   "metadata": {},
   "source": [
    "The reason why i have decided to us the import pandas as pd - asp_failure_df is to handle any missing or non-numerica value in the dataset am currently analysing, the aim of this code is to replace all instances of string 'na' with a numerica value of 0 in the entire dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8da677c",
   "metadata": {},
   "source": [
    "By replacing the 'na' with a 0 this will make the data suitable for calculations, statisical analysis. which will allow the data to avoid any unecessary issues related to non-numeric or missing values in my analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0055bebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df=aps_failure_df.drop(aps_failure_df.columns[aps_failure_df.isnull().mean()>10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67051756",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df.fillna(aps_failure_df.median(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a101b2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf5a1ec",
   "metadata": {},
   "source": [
    "# Re-naming "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef2555d",
   "metadata": {},
   "source": [
    "Renaming my verables or features, I have decided not to take this option as the dataset i have am happy to identfy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf969628",
   "metadata": {},
   "source": [
    "# Clearned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6ce665",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af66d091",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis - Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b4e14c",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis or (EDA) is understanding the data set by summarizing its main characteristics and often plotting them visually. This step is very important especially when we arrive at modelling the data to apply Machine learning. Plotting in EDA consists of Histograms, Box plot, Scatter plots and many more.\n",
    "\n",
    "Through the process of EDA, we can also refine the problem statement or definition of our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db851529",
   "metadata": {},
   "source": [
    "# Boxplot Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a89cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statistics as Stat\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289bc592",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=aps_failure_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ae649a",
   "metadata": {},
   "source": [
    "Looking at diagram ca_000 to da_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48da0a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=aps_failure_df['ca_000'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e737d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=aps_failure_df['da_000'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fe26b4",
   "metadata": {},
   "source": [
    "Looking at diagram ee_000 to aa_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88218b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=aps_failure_df['ee_000'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10095475",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=aps_failure_df['aa_000'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f4ef08",
   "metadata": {},
   "source": [
    "# Scatter Chart for Random Range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77c0a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 171\n",
    "x = np.random.rand(N)\n",
    "y = np.random.rand(N)\n",
    "colors = np.random.rand(N)\n",
    "area = (30 * np.random.rand(N))**2  # 0 to 15 point radii\n",
    "\n",
    "plt.scatter(x, y, s=area, c=colors, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ef8e77",
   "metadata": {},
   "source": [
    "The purpose of this code is to create a scatter plot with some specific charaterticis. \n",
    "As you can see with the scatter plot allow the visualistion of the data point in two-dimenisional space. in this case x & y represent the coordinates of this point. \n",
    "\n",
    "I decided to use the ramndon data, to test the visualisation with random data.\n",
    "\n",
    "The Scatter plot has generated 171 data points, where each point jas a random position (x,y) a ransome size. The scatter plot will clearly show these point are distrubuted in two-dimenal space. I believe its a useful tool for exploring data, checking paterns and unsertanding variables may relate to eachother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701d72e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count = aps_failure_df[\"class\"].value_counts()\n",
    "print(class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8391af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = pd.Series([5900, 1000], index=['Negative', 'Positive'])\n",
    "plt.figure(figsize=(6, 4))\n",
    "class_counts.plot(kind= 'bar', color=['Blue', 'purple'])\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Units')\n",
    "plt.title('Air Pressur system-Apps Failure')\n",
    "plt.xticks(range(len(class_count.index)), ['Negative', 'Positive'], rotation=0)\n",
    "plt.legend(['The Nagative is not Related to Air Pressure system'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7429dce",
   "metadata": {},
   "source": [
    "In this sections i will be covering \"what data are we exploring in this assignment, I will be looking\n",
    "\n",
    "We are going to look at a data set on cars called “cardata.csv”.\n",
    "\n",
    "The data contains more than 6,000 rows and more than 171 columns which have features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f745795b",
   "metadata": {},
   "source": [
    "Assessing a Specific col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af297339",
   "metadata": {},
   "source": [
    "# Step 4: Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47007327",
   "metadata": {},
   "source": [
    " Principal Component Analysis, In this section, i will be explore what is perhaps one of the most broadly used of unsupervised algorithms, principal component analysis (PCA). PCA is fundamentally a dimensionality reduction algorithm, but it can also be useful as a tool for visualization, for noise filtering,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b663e5",
   "metadata": {},
   "source": [
    "# Number of Features Needed for Retain 99.5% Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b6c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "L_encoder = LabelEncoder()\n",
    "aps_failure_df[\"class\"]=L_encoder.fit_transform(aps_failure_df[\"class\"])\n",
    "aps_failure_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e133ea",
   "metadata": {},
   "source": [
    "The code above am using scikit-learn library to encove categorical lables into nume3rical vales as this is my goal in specififcally, coverting the class column in the aps_failure_df. The main purpose is tp prepare the target veriables for machine learning algorithms. The labelEncoder will assigne a unique number to each unique class column making it easier ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b3a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b9b104",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df_no_label = aps_failure_df.drop(aps_failure_df[\"class\"])\n",
    "pca = PCA().fit(aps_failure_df_no_label) \n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_)) \n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.axhline(0.9950,color= \"orangered\",alpha=.5,ls=\"--\")\n",
    "plt.axvline(2,color= \"orangered\",alpha=.5,ls=\"--\");\n",
    "plt.xlim(0,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfaf9ae",
   "metadata": {},
   "source": [
    "The main aim is to retain 99.5% variance data from the graphic show above, my understanding is that i must keep my features no higher or below 2 in order to get the results i am looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e79f31",
   "metadata": {},
   "source": [
    "The Graph above clearly show's that i would need it to be 2 Components to achive my objective of hitting a 99.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46b5924",
   "metadata": {},
   "source": [
    "Analysing the implement PCA to dimensionally reduce the data to the number of features that you have discovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860d5842",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(2)\n",
    "projected = pca.fit_transform(aps_failure_df_no_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6554d399",
   "metadata": {},
   "outputs": [],
   "source": [
    "projected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204ee830",
   "metadata": {},
   "source": [
    "Side notes: observatins (Rows) equalling to 59998 and having 10 columns (Attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff53f1c8",
   "metadata": {},
   "source": [
    "# Identfying Array's within the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12694df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "projected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3cb5e3",
   "metadata": {},
   "source": [
    "The Array's format clearly displays that each of my obsersavtion has number of values of 2, although not the orginal values were present in the orginal dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5809a2f1",
   "metadata": {},
   "source": [
    "in order to understand the dataframe better and have a clear picture i have clearly displayed below: for the purpose of demonstrations, I have labled the colums all 10 of them below for visual purpose. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3afcc1",
   "metadata": {},
   "source": [
    "# Identfying PCA Ptojected Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da34fd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_pca = pd.DataFrame(projected, columns =['C1','C2',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebd1b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82324062",
   "metadata": {},
   "source": [
    "As you can see that all my columns are labled with C1 to C10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_pca[\"class\"] = aps_failure_df[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36749d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348d62da",
   "metadata": {},
   "source": [
    "Data set I want to work with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f710b72",
   "metadata": {},
   "source": [
    "As you can see that I will be selecting only the Frist Colum from C1 to C2. just before it get to the last column class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d657d2",
   "metadata": {},
   "source": [
    "PCA was used to establish the minimum number of features needed for retaining 99.5% variance in the data and then implement PCA to dimensionally reduce the data to the number of features that you have discovered. \n",
    "\n",
    "This will Include a clear and concise explanation of what you i will be doing with the data \n",
    "\n",
    "and why you are doing it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b024aaa",
   "metadata": {},
   "source": [
    "# Step 5:Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c001e3",
   "metadata": {},
   "source": [
    "I believe that curse of dimensionality referes to limited the issue of having limited obeservations, that being said having a resonable features. However, in order to identify the pattern within the dataset provided aps_failure_df, the options of requiring to removing the features which will allow the increase of observations which will lead the analysis a clear roadmap f the dataset. \n",
    "\n",
    "To achieve this, Princial component anlysis will play a crurial role, Although one must take in to consideration that one must implemente the cleaning stage before hand which reqires the Exploratory Data Analysis. With that process, this has highlighted the opertunity to remove the feature Class. This process is a must if the data would remain accurate dataset.I believe that this is a must process end steps in order to plan for appling michine learning. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa7bac7",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c633bda",
   "metadata": {},
   "source": [
    "in conclusion, I believe that the data preperation has been very crucial component of my analysis, laying down the foundation for my subsequent data drive assignment. Thoughout this assignment, I have been meticulous with cleaning the raw data to ensure its quality importing  from the library, The process involved serveral key steps, including loading data from the library, cleaning, handling missing data.\n",
    "\n",
    "Continuing with cleaning and analysing the dataset, I had found that there had still columns with the catagory value of \"na\" & \"nan\" which i have identified as not being value which will cause some issues in the long run. \n",
    "\n",
    "The only solution for this dilema is to change the value with a \"uniqu feature\". \n",
    "\n",
    "In order to achieve this, I have implement the EDA process by indentifing all columns with more than >10% missing value would be dropped and replaced with the medium.\n",
    "\n",
    "The nest steps i took was with the Exploratory Data Analysis - Visualisation, I have taken the optertunity to use multiple visualisation graphs which are listed below: \n",
    "\n",
    "1. (5) Boxplot: My boxplot clearly indicates that there are outliers on the top of the graphs, with random features.\n",
    "      1.  [ca_000 to da_000]\n",
    "      2.   [ee_000 to aa_000]\n",
    "      \n",
    "      \n",
    "2. Scatter Char (1) Indicates the relationship between the different features. Athough in this case, I am looking in to random features. \n",
    "\n",
    "\n",
    "3. Histogram: (1) : The reason for histogram graph is to demostrate the differance units regarding the \"Negative\" & \"Postive\" values that are within the targetted veriables class. This process was crucial as it indicated an insight to what  was and what was not connected to the components failures for the ASP system.\n",
    "\n",
    "\n",
    "Finally, the columns class was labelled from category value to numerical vaule in order to start the PCA process.\n",
    "\n",
    "\n",
    "Implementing Princiaple Component Analysis not only assisted, has also highlighted the how many features that I would need to retain 99.5% of the cumulative explained variance of the dataset.\n",
    "\n",
    "As you can clearly see in the graph that it shows in order to retain the 99.5% i would need to us (2) numbers of components with the array to lolcate the pattern from the dataset its self to reach my aimed result.\n",
    "\n",
    "I honest Believe that these steps were crucial with supporting my analysis. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28294bea",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae13f5f",
   "metadata": {},
   "source": [
    "1.\tCamp, D. (2023). Pandas Read csv() Tutorial: Importing Data. [online] https://www.datacamp.com/tutorial/pandas-read-csv. Available at: https://www.datacamp.com/tutorial/pandas-read-csv.\n",
    "\n",
    "2.\tGeeksforgeeks (2018a). Get unique values from a column in Pandas DataFrame. [online] GeeksforGeeks. Available at: https://www.geeksforgeeks.org/get-unique-values-from-a-column-in-pandas-dataframe/ [Accessed 10 Dec. 2018].\n",
    "\n",
    "3.\tGeeksforgeeks (2018b). Pandas isnull() and notnull() Method. [online] GeeksforGeeks. Available at: https://www.geeksforgeeks.org/python-pandas-isnull-and-notnull/?ref=gcse [Accessed 1 Nov. 2023].\n",
    "\n",
    "4.\tIdris, I. (2011). NumPy 1.5 : beginner’s guide. Birmingham: Packt Publishing.\n",
    "\n",
    "5.\tJohn Paul Mueller and Luca Massaron (2023). Python for Data Science For Dummies. John Wiley & Sons.\n",
    "\n",
    "6.\tNik (2021). Pandas: Number of Rows in a Dataframe (6 Ways) • datagy. [online] datagy. Available at: https://datagy.io/pandas-number-of-rows/.\n",
    "\n",
    "7.\tPandas (2023a). Chart Visualization — pandas 1.3.3 documentation. [online] pandas.pydata.org. Available at: https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html.\n",
    "\n",
    "8.\tPandas (2023b). pandas.DataFrame.describe — pandas 1.0.3 documentation. [online] pandas.pydata.org. Available at: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html.\n",
    "\n",
    "9.\tPandas (2023c). Working with missing data — pandas 1.2.4 documentation. [online] pandas.pydata.org. Available at: https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html.\n",
    "\n",
    "10.\tPratusevich, M. (2017a). Practice Python. [online] Practicepython.org. Available at: https://www.practicepython.org/.\n",
    "\n",
    "11.\tPratusevich, M. (2017b). Practice Python. [online] Practicepython.org. Available at: https://www.practicepython.org/.\n",
    "\n",
    "12.\tSaravanan (2021). Import Pandas as pd. [online] Medium. Available at: https://medium.com/analytics-vidhya/import-pandas-as-pd-26344cd7235e.\n",
    "\n",
    "13.\tScikit Learn (2009). sklearn.decomposition.PCA — scikit-learn 0.20.3 Documentation. [online] Scikit-learn.org. Available at: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html [Accessed 2023].\n",
    "\n",
    "14.\tSeaborn (2023). seaborn.boxplot — seaborn 0.11.1 documentation. [online] seaborn.pydata.org. Available at: https://seaborn.pydata.org/generated/seaborn.boxplot.html.\n",
    "\n",
    "15.\tStack Overflow (2014a). Is there a way to determine the order of labels in scikit-learn’s LabelEncoder? [online] Stack Overflow. Available at: https://stackoverflow.com/questions/25343411/is-there-a-way-to-determine-the-order-of-labels-in-scikit-learns-labelencoder [Accessed 1 Nov. 2023].\n",
    "\n",
    "16.\tStack Overflow (2014b). Python - Principal Components Analysis Using Pandas Dataframe. [online] Stack Overflow. Available at: https://stackoverflow.com/questions/23282130/principal-components-analysis-using-pandas-dataframe.\n",
    "\n",
    "17.\tStack Overflow (2018). understand df.isnull.mean() in python. [online] Stack Overflow. Available at: https://stackoverflow.com/questions/53283142/understand-df-isnull-mean-in-python.\n",
    "\n",
    "18.\tStack Overflow (2019). How to run Python Code with ‘%matplotlib inline’? [online] Stack Overflow. Available at: https://stackoverflow.com/questions/55687832/how-to-run-python-code-with-matplotlib-inline.\n",
    "\n",
    "19.\tStack Overflow (2021). How to Generate Random Colors in matplotlib? [online] Stack Overflow. Available at: https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib [Accessed 1 Nov. 2023].\n",
    "\n",
    "20.\tStack overflow (2015). Python scikit learn pca.explained_variance_ratio_ cutoff. [online] Stack Overflow. Available at: https://stackoverflow.com/questions/32857029/python-scikit-learn-pca-explained-variance-ratio-cutoff.\n",
    "21.\tStack overflow (2018). Understanding inplace=True in pandas. [online] Stack Overflow. Available at: https://stackoverflow.com/questions/43893457/understanding-inplace-true-in-pandas [Accessed 1 Nov. 2023].\n",
    "\n",
    "22.\tStackExchange (2019). What Does the PCA().transform() Method do? [online] Cross Validated. Available at: https://stats.stackexchange.com/questions/409176/what-does-the-pca-transform-method-do [Accessed 1 Nov. 2023].\n",
    "\n",
    "23.\tStackOverflow (2012). How to run Python Code with ‘%matplotlib inline’? [online] Stack Overflow. Available at: https://stackoverflow.com/questions/55687832/how-to-run-python-code-with-matplotlib-inline [Accessed 1 Nov. 2023].\n",
    "\n",
    "24.\tStackoverflow (2011). r - Subset of rows containing NA (missing) values in a chosen column of a data frame. [online] Stack Overflow. Available at: https://stackoverflow.com/questions/7980622/subset-of-rows-containing-na-missing-values-in-a-chosen-column-of-a-data-frame.\n",
    "\n",
    "25.\tStackoverflow (2016). How to resolve AttributeError: ‘DataFrame’ object has no attribute. [online] Stack Overflow. Available at: https://stackoverflow.com/questions/38134643/how-to-resolve-attributeerror-dataframe-object-has-no-attribute [Accessed 1 Nov. 2023].\n",
    "\n",
    "26.\tStackoverflow (2021). Python - isnull().sum() vs isnull().count(). [online] Stack Overflow. Available at: https://stackoverflow.com/questions/60249807/python-isnull-sum-vs-isnull-count [Accessed 1 Nov. 2023].\n",
    "\n",
    "27.\tVishal (2019). Python Basic Exercise for Beginners. [online] PYnative. Available at: https://pynative.com/python-basic-exercise-for-beginners/.\n",
    "\n",
    "28.\tw3school (2023). Pandas DataFrame head() Method. [online] www.w3schools.com. Available at: https://www.w3schools.com/python/pandas/ref_df_head.asp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bf6d56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
