{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "892e7d6c",
   "metadata": {},
   "source": [
    "# Hodan Mohamed Abdi SBA23416_Data Preperation_CA1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1fe033",
   "metadata": {},
   "source": [
    "# Characterisation of the data set- aps_failure_set (1).csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f7a36d",
   "metadata": {},
   "source": [
    "<font color=\"blue\"> Introduction: \n",
    "\n",
    "My assignment i have been asked by Haulage company to analyse a dataset based on data collected from heavy Scania trucks in everyday usage. The system in focus is the Air Pressure system (APS) which generates pressurised air that are utilized in various functions in a truck, such as braking and gear changes.\n",
    "\n",
    "The dataset’s <font color=\"green\"> \"Positive Class\"</font> consists of component failures for a specific component of the APS system.  \n",
    "\n",
    "The dataset's <font color=\"red\"> \"Negative Class\"</font> consists of trucks with failures for components not related to the APS\n",
    "\n",
    "The main aim of this analysis will help me determine the investment strategy for the company in the upcoming year.\n",
    "\n",
    "All data wrangling, analysis, and visualizations must be generated using python / Notebook Jupiter and various libraries.\n",
    "\n",
    "The companies CTO also requires that i must include rationalize all the decisions that you have made in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b173cc6",
   "metadata": {},
   "source": [
    "Assignment Minimum Requirements: \n",
    "\n",
    "<font color=\"blue\"> Step 1:\n",
    "    \n",
    "The assignment is to: use the dataset contained within the file “aps_failure_set.csv”, conduct the following analysis and report with my findings as following step below:\n",
    "\n",
    "Characterisation of the data set: Which incldues the size; How many number of attributes; if there are or has/does not have missing values, Also listing the number of observations. and idenfying what these Characterisation mean.\n",
    "\n",
    "I will be importing the dataset with Panda library to verify what are the specific's\n",
    "\n",
    "<font color=\"blue\"> Step 2:\n",
    "    \n",
    "Application of Data preparation & Evaluation Methods: Include the following:Cleaning, renaming, & Exploratory Data Analysis (EDA) to get a better understanding of the data-set by summarizing its main characteristics and often plotting them visually.\n",
    "\n",
    "I have come to understand that this step is very important especially when we arrive at modelling the data to apply Machine learning. Plotting in EDA consists of Histograms, Box plot, Scatter plots.\n",
    "\n",
    "<font color=\"blue\"> Step 3:\n",
    " \n",
    "Use Principal Component Analysis, In this section, i will be explore what is perhaps one of the most broadly used of unsupervised algorithms, principal component analysis (PCA). PCA is fundamentally a dimensionality reduction algorithm, but it can also be useful as a tool for visualization, for noise filtering,\n",
    "\n",
    "<font color=\"blue\">Step 4:\n",
    "\n",
    "Final Part: Curse of Dimensionality & Conclusion, I will end the assignment with my finding toward my analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25770cc1",
   "metadata": {},
   "source": [
    "# 1. Characterisation of the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa327e5",
   "metadata": {},
   "source": [
    "The First Step is to identify which libraries i will be using in order to import my dataset /dataframe. I have decided the best route would be to use \"Import Pandas\" & reading from <font color=\"green\">aps_failure_df =pd.read_csv (\"aps_failure_set (2).csv\")text</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9be815ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "sns.set() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f3272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df =pd.read_csv (\"aps_failure_set (2).csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f555d84",
   "metadata": {},
   "source": [
    "<font color=\"green\"> Size of the dataset</font> : In this section i have identified the size of the dataset using the follow code below: Dataset size: (6000 , 171)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7e5d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = aps_failure_df.shape\n",
    "print(\"Dataset Size:\", size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb143ca",
   "metadata": {},
   "source": [
    "<font color=\"green\"> Data Information</font>: In this section, I have highlighted the current datafrane which includes the following, range index, columns, dtypes and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81912918",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193f353b",
   "metadata": {},
   "source": [
    "To get a better understanding of the dataset, I would need to identify rhe Row's and Columns and displaying the dtype value counts. as you can cledarly see within the below that the has 6000 Rows and 171 Colums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98e9e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The data set has {} rows and {} columns\".format(aps_failure_df.shape[0], aps_failure_df.shape[1]))\n",
    "display(aps_failure_df.describe())\n",
    "display(aps_failure_df.head())\n",
    "print(aps_failure_df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1664c847",
   "metadata": {},
   "source": [
    "I have Identified the following: \n",
    " \n",
    "   <font color=\"Green\"> • Object = 171</font>\n",
    "   \n",
    "   <font color=\"green\"> •Int64 = 1</font>\n",
    "   \n",
    "   <font color=\"green\">   •dtype = int64</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebb4ab0",
   "metadata": {},
   "source": [
    "# Identfying Atributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab4ea1c",
   "metadata": {},
   "source": [
    "In summary, identifying attributes and observations is a critical step in data analysis and modeling. It forms the basis for understanding the dataset's structure, selecting relevant features, and ensuring data quality. This information enables informed decisions, helps with feature engineering, and ultimately leads to more accurate and effective data analysis and modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e124bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attributes = len(aps_failure_df.columns)\n",
    "print(\"Number of Attributes:\", num_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecac661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_observations = aps_failure_df.shape[0]\n",
    "print(\"Number of Observations:\", num_observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dd515a",
   "metadata": {},
   "source": [
    "# Identfying Dublicate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b933104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc29ab66",
   "metadata": {},
   "source": [
    "Zero Dublication were found in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a512d26f",
   "metadata": {},
   "source": [
    "# Identyfing Missing data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa17028",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c1f7bd",
   "metadata": {},
   "source": [
    "The class column contains the labels and its appears that the labels are being represented with the value of Zero’s\n",
    "\n",
    "    •Lengh Equals to 171\n",
    "\n",
    "    •Data Types equales to int64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b20f6b",
   "metadata": {},
   "source": [
    "# Idenfying what these Characterisation mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5519a232",
   "metadata": {},
   "source": [
    "The dataset i'm currently analysing consist of 600 rows (Observatios) & 171 Colums (Attributes), This makes it a modertely large dataset. Among these attributes, there are 171 distinc ones.\n",
    "\n",
    "In my analysis, I'm taking steps to identify the duplicated data entries, which helps maintain the datasets intergrity. Addtionally, I will be addrressing the missing values  (Na, NAN) to ensure that any fields with non-numeric or null vales are appropriatley transformed into numerica data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f4f7b1",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9537f75e",
   "metadata": {},
   "source": [
    "Exploratory data analysis I have found to be a curitical step in the data analysis process for serval important reasons are as understanding the dataset, this will allow me to explore thestructure, conects and charartics of the data. EDA process will hopefully allow me to identfy the patterns and relationships within my dataset, this will also allow myself to spot trends, anomalies and dependencies between variabl that might not be apparent at fist. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b42f3",
   "metadata": {},
   "source": [
    "Data Quality Assurance, will help me identify and address data quality issues such as missing values, outliers and inconsistencies. By addressing this early will enable myself to ensure the anaylsis is clean and reliable. Below as you can see I have deployed aps_failure_df.describe(include =\"object\") to identified the NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6a6aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df.describe(include =\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa7dc64",
   "metadata": {},
   "source": [
    "As displayed above dataset, I have identified that there are dataset with NA in the column of ab_000, ad_000 which i have conclued that this would require to bechanged, although i still need to analys more of my dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0066853a",
   "metadata": {},
   "source": [
    "# Application of Data preparation & Evaluation methods "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c1c940",
   "metadata": {},
   "source": [
    "# Cleaning Dataset Process "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e204fa",
   "metadata": {},
   "source": [
    "This part of the process I will be working on identifying the unqiue value as well as the isnull values. As you can see thatdateframe: aps.failure_df has shown that there are quit a few datasets that are unique and now we can move to identifying isnull dataset to unsure that am working with consistant datasets.\n",
    "\n",
    "looking in to: [ \"ab_000\"] & [\"ad_000\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff582db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df[\"ab_000\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85362db",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df[\"ab_000\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b5ed82",
   "metadata": {},
   "source": [
    "As you can see above that there is 0 isnull with sum. am happy to continue with the cleaning of my dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964d466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df[\"ad_000\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf09cdc",
   "metadata": {},
   "source": [
    "In order to define the unique values within my dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cf78d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df[\"ad_000\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6872d41",
   "metadata": {},
   "source": [
    "Next Step is to identfy the missing value using the missing_value as this will be valuable for the dataset analysis preprocessing. It is important to identfy the missing values in each columns (Attributes) within my dataset. The reason for this process as i would agree that its very much criticial as it helps me understand the qualitgy of dataset and also highlighting key area's that require cleaning or renaming dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eba146",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values=[\"NA\",\"N/A\", \"-\", \"NaN\", \"missing\", \"na\", \"?\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362e769c",
   "metadata": {},
   "source": [
    "In order to identify and locate any missing value, i have decided to use the missing_value = ['na'] as part of my dataset cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3adc101",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df=pd.read_csv(\"aps_failure_set (2).csv\" , na_values=missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ced077",
   "metadata": {},
   "source": [
    "My next step to chaning the missing values to number will be as followed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5a6f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value=aps_failure_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75acf630",
   "metadata": {},
   "source": [
    "In order to identify and quantify the missing vaules within APS_Failure_DF, I need this code to calculate the number of missing vailes for each column and store the results in the missing_value \"variables\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45a0a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(missing_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e8b700",
   "metadata": {},
   "source": [
    "As you can see that the missing valued has now been replaced with unique values with the application of using EDA method, Although that been said, With the remaining columns that are no longer required can be removed. \n",
    "by doing this i have gained the valuable insight into the data quality which will allow me to take informed action.\n",
    "\n",
    "It's key to understand which columns have missing data, especially how much data is missing in each column. this allows myself to take action that will be beneficial for my dataset and results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e85a0c",
   "metadata": {},
   "source": [
    "# Removing - Droping Values - Changing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400f71ff",
   "metadata": {},
   "source": [
    "This Steps will disucss and highlight the process of removing the columns are as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d87d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "aps_failure_df = aps_failure_df.replace('na', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f85fce",
   "metadata": {},
   "source": [
    "The reason why i have decided to us the import pandas as pd - asp_failure_df is to handle any missing or non-numerica value in the dataset am currently analysing, the aim of this code is to replace all instances of string 'na' with a numerica value of 0 in the entire dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7216f815",
   "metadata": {},
   "source": [
    "By replacing the 'na' with a 0 this will make the data suitable for calculations, statisical analysis. which will allow the data to avoid any unecessary issues related to non-numeric or missing values in my analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0055bebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df=aps_failure_df.drop(aps_failure_df.columns[aps_failure_df.isnull().mean()>10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ff7898",
   "metadata": {},
   "source": [
    "I have noticed that i was receving warning signs and decided to remove or ignore them with the following below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f754661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67051756",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df.fillna(aps_failure_df.median(),inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a054c7",
   "metadata": {},
   "source": [
    "Next step to identfy the describe dataset within mean function above which has resulted in below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a101b2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5c9738",
   "metadata": {},
   "source": [
    "# Re-naming "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab261a9",
   "metadata": {},
   "source": [
    "Renaming my verables or features, I have decided not to take this option as the dataset i have am happy to identfy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a2b6ae",
   "metadata": {},
   "source": [
    "# Clearned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6ce665",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af66d091",
   "metadata": {},
   "source": [
    "# Step 3: Exploratory Data Analysis - Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b4e14c",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis or (EDA) is understanding the data set by summarizing its main characteristics and often plotting them visually. This step is very important especially when we arrive at modelling the data to apply Machine learning. Plotting in EDA consists of Histograms, Box plot, Scatter plots and many more.\n",
    "\n",
    "Throughout the process of EDA, I can also refine the problem statement or definition of our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff80c98",
   "metadata": {},
   "source": [
    "# Boxplot Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b2e4dd",
   "metadata": {},
   "source": [
    "I have Identified use of (5) Boxplot: My boxplot clearly indicates that there are outliers on the top of the graphs, with random features.\n",
    "\n",
    "[ca_000 to da_000]\n",
    "[ee_000 to aa_000]\n",
    "\n",
    "To identfy and highligh any relatioship as well showing any \"Outliers\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabd0a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statistics as Stat\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289bc592",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=aps_failure_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076849cb",
   "metadata": {},
   "source": [
    "                                                                                            Graph 1: Full Datasets Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9e391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=aps_failure_df['ca_000'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9697e69",
   "metadata": {},
   "source": [
    "                                                                            Graph 2:sns.boxplot(x=aps_failure_df['ca_000'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09bfa19",
   "metadata": {},
   "source": [
    "Looking at diagram ca_000 to da_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d23629",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=aps_failure_df['da_000'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a32e3",
   "metadata": {},
   "source": [
    "                                                                         Graph 3:sns.boxplot(x=aps_failure_df['da_000'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3276c0",
   "metadata": {},
   "source": [
    "Looking at diagram ee_000 to aa_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899e9ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=aps_failure_df['ee_000'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53af554",
   "metadata": {},
   "source": [
    "                                                                            Graph 4:sns.boxplot(x=aps_failure_df['ee_000'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f5650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=aps_failure_df['aa_000'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367d5435",
   "metadata": {},
   "source": [
    "                                                                            Graph 5:sns.boxplot(x=aps_failure_df['aa_000'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c799fb0",
   "metadata": {},
   "source": [
    "The reason for using the boxplot was to repersentation of the relationship between the following [\"ca_000\" to \"da_000\"] & [\"ee_000\"] to [\"aa_000\"] All the diagrams have display \"outliars\" which are easy to identify. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281401f7",
   "metadata": {},
   "source": [
    "# Scatter Chart for Random Range "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b79682",
   "metadata": {},
   "source": [
    "The purpose of this code is to create a scatter plot with some specific charaterticis. \n",
    "As you can see with the scatter plot allow the visualistion of the data point in two-dimenisional space. in this case x & y represent the coordinates of this point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b95e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 171\n",
    "x = np.random.rand(N)\n",
    "y = np.random.rand(N)\n",
    "colors = np.random.rand(N)\n",
    "area = (40 * np.random.rand(N))**2  \n",
    "\n",
    "plt.scatter(x, y, s=area, c=colors, alpha=0.5)\n",
    "plt.title('Air Pressur system-Apps Failure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388c3bfe",
   "metadata": {},
   "source": [
    "                                                                                    Graph 6: plt.scatter-Random Range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeb071e",
   "metadata": {},
   "source": [
    "I have decided to use the ramndon data, to test the visualisation with random data. in order to complete this i would need to promtp / implement \"Scatter Plot Chart\". \n",
    "\n",
    "The Scatter plot has generated 171 data points, where each point jas a random position (x,y) a ransome size. The scatter plot will clearly show these point are distrubuted in two-dimenal space. I believe its a useful tool for exploring data, checking paterns and unsertanding variables may relate to eachother.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d24e6f8",
   "metadata": {},
   "source": [
    "# Visualisation: Histagram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edbcc14",
   "metadata": {},
   "source": [
    "In this section I have decided to identify the component that are with the <font color=\"red\">Negative</font>  and <font color=\"green\">Postive</font> and in order to establish this, i will be taking few steps to identfy this. \n",
    "\n",
    "First Step: Using the Print function:  <font color=\"purple\"> print(aps_failure_df[\"class\"].value_counts()) text.</font>\n",
    "\n",
    "*The dataset’s <font color=\"green\">positive class</font>  consists of component failures for a specific component of the APS system.\n",
    "\n",
    "*The Dataset  <font color=\"red\"> \"Negative Class\"</font> consists of trucks with failures for components not related to the APS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b8a854",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aps_failure_df[\"class\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7b8ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count = aps_failure_df[\"class\"].value_counts()\n",
    "print(class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbdfc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = pd.Series([5900, 1000], index=['Negative', 'Positive'])\n",
    "plt.figure(figsize=(6, 4))\n",
    "class_counts.plot(kind= 'bar', color=['Blue', 'purple'])\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Units')\n",
    "plt.title('Air Pressure system-Apps Failure')\n",
    "plt.xticks(range(len(class_count.index)), ['Negative', 'Positive'], rotation=0)\n",
    "plt.legend(['The Nagative is not Related to Air Pressure system'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75b4551",
   "metadata": {},
   "source": [
    "                                                                                Graph 6: Air Pressure System - Apps Failure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7429dce",
   "metadata": {},
   "source": [
    "In this sections i will be covering the values of the negatives & postives regarding the vaules, which clearly shown within the features \"Class\", As you can see above i have identified the clas_value: \n",
    "\n",
    "0 Equaling to being a <font color=\"red\"> \"Negative Class\"</font>  & 1 Equaling to a <font color=\"green\"> \"Positive Class\"</font>. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af297339",
   "metadata": {},
   "source": [
    "# Step 4: Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47007327",
   "metadata": {},
   "source": [
    " Principal Component Analysis, In this section, i will be explore what is perhaps one of the most broadly used of unsupervised algorithms, principal component analysis (PCA). PCA is fundamentally a dimensionality reduction algorithm, but it can also be useful as a tool for visualization, for noise filtering,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044162a4",
   "metadata": {},
   "source": [
    "# Number of Features Needed for Retain 99.5% Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d878593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "L_encoder = LabelEncoder()\n",
    "aps_failure_df[\"class\"]=L_encoder.fit_transform(aps_failure_df[\"class\"])\n",
    "aps_failure_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c0284b",
   "metadata": {},
   "source": [
    "The code above am using scikit-learn library to encove categorical lables into numerical vales as this is my goal in specififcally, coverting the class column in the aps_failure_df. The main purpose is tp prepare the target veriables for machine learning algorithms. The labelEncoder will assigne a unique number to each unique class column making it easier ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b3a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b9b104",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failure_df_no_label = aps_failure_df.drop(aps_failure_df[\"class\"])\n",
    "pca = PCA().fit(aps_failure_df_no_label) \n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_)) \n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.title('Explained Variance Ratio')\n",
    "plt.axhline(0.9950,color= \"orangered\",alpha=.5,ls=\"--\")\n",
    "plt.axvline(2,color= \"orangered\",alpha=.5,ls=\"--\");\n",
    "plt.xlim(0,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c40d22",
   "metadata": {},
   "source": [
    "                                                                                          Graph 7: Explained Variance Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfaf9ae",
   "metadata": {},
   "source": [
    "The main aim is to retain 99.5% variance data from the graphic show above, my understanding is that i must keep my features no higher or below 2 in order to get the results i am looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b7393b",
   "metadata": {},
   "source": [
    "The Graph above clearly show's that i would need it to be 2 Components to achive my objective of hitting a 99.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3876fc6c",
   "metadata": {},
   "source": [
    "Analysing the implement PCA to dimensionally reduce the data to the number of features that you have discovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860d5842",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(2)\n",
    "projected = pca.fit_transform(aps_failure_df_no_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6554d399",
   "metadata": {},
   "outputs": [],
   "source": [
    "projected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204ee830",
   "metadata": {},
   "source": [
    "Side notes: observatins (Rows) equalling to 59998 and having 10 columns (Attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cb8fc5",
   "metadata": {},
   "source": [
    "# Identfying Array's within the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12694df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "projected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3cb5e3",
   "metadata": {},
   "source": [
    "The Array's format clearly displays that each of my obsersavtion has number of values of 2, although not the orginal values were present in the orginal dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5809a2f1",
   "metadata": {},
   "source": [
    "in order to understand the dataframe better and have a clear picture i have clearly displayed below: for the purpose of demonstrations, I have labled the colums all 10 of them below for visual purpose. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2f1e66",
   "metadata": {},
   "source": [
    "# Identfying PCA Projected Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850fb279",
   "metadata": {},
   "source": [
    "In this section i will be looking at the projected colums which i have identified with the following fuctions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da34fd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_pca = pd.DataFrame(projected, columns =['C1','C2',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebd1b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82324062",
   "metadata": {},
   "source": [
    "As you can see that all my columns are labled with C1 to C10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_pca[\"class\"] = aps_failure_df[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36749d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348d62da",
   "metadata": {},
   "source": [
    "Data set I want to work with are C1 & C2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f710b72",
   "metadata": {},
   "source": [
    "As you can see that I will be selecting only the Frist Colum from C1 to C2. just before it get to the last column class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d657d2",
   "metadata": {},
   "source": [
    "PCA was used to establish the minimum number of features needed for retaining 99.5% variance in the data and then implement PCA to dimensionally reduce the data to the number of features that you have discovered. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b024aaa",
   "metadata": {},
   "source": [
    "# Step 5:Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1e312f",
   "metadata": {},
   "source": [
    "I believe that curse of dimensionality referes to limited the issue of having limited obeservations, that being said having a resonable features. However, in order to identify the pattern within the dataset provided aps_failure_df, the options of requiring to removing the features which will allow the increase of observations which will lead the analysis a clear roadmap f the dataset. \n",
    "\n",
    "To achieve this, Princial component anlysis will play a crurial role, Although one must take in to consideration that one must implemente the cleaning stage before hand which reqires the Exploratory Data Analysis. With that process, this has highlighted the opertunity to remove the feature Class. This process is a must if the data would remain accurate dataset.I believe that this is a must process end steps in order to plan for appling michine learning. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa7bac7",
   "metadata": {},
   "source": [
    "# Step 5: Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfdcb0a",
   "metadata": {},
   "source": [
    "In conclusion, I believe that the data preperation has been very crucial component of my analysis, laying down the foundation for my subsequent data drive assignment. Thoughout this assignment, I have been meticulous with cleaning the raw data to ensure its quality importing  from the library, The process involved serveral key steps, including loading data from the library, cleaning, handling missing data.\n",
    "\n",
    "Continuing with cleaning and analysing the dataset, I had found that there had still columns with the catagory value of \"na\" & \"nan\" which i have identified as not being value which will cause some issues in the long run. \n",
    "\n",
    "The only solution for this dilema is to change the value with a \"uniqu feature\". \n",
    "\n",
    "In order to achieve this, I have implement the EDA process by indentifing all columns with more than >10% missing value would be dropped and replaced with the medium.\n",
    "\n",
    "The next steps i took was with the Exploratory Data Analysis - Visualisation, I have taken the optertunity to use multiple visualisation graphs which are listed below: \n",
    "\n",
    "1. I have Identified use of (5) Boxplot: My boxplot clearly indicates that there are outliers on the top of the graphs, with random features.\n",
    "      1.  [ca_000 to da_000]\n",
    "      2.   [ee_000 to aa_000]\n",
    "      \n",
    "      \n",
    "2. I have Identified use of Scatter Char (1) Indicates the relationship between the different features. Athough in this case, I am looking in to random features. \n",
    "\n",
    "\n",
    "3. I have Identified use of Histogram: (1) : The reason for histogram graph is to demostrate the differance units regarding the \"Negative\" & \"Postive\" values that are within the targetted veriables class. This process was crucial as it indicated a high level insight to what was and what was not connected to the components failures for the ASP system.\n",
    "\n",
    "\n",
    "Finally, The columns class was labelled from category value to numerical vaule in order to start the PCA process withput this proocess it would have been difficult to analys the dataset..\n",
    "\n",
    "\n",
    "Implementing Princiaple Component Analysis not only assisted, It has also highlighted the how many features that I would need to retain 99.5% of the cumulative explained variance of the dataset in the diagram.\n",
    "\n",
    "As you can clearly see in the graph that it shows in order to retain the 99.5% i would need to us (2) numbers of components with the array to lolcate the pattern from the dataset its self to reach my aimed result.\n",
    "\n",
    "I honest Believe that these steps were crucial with supporting my analysis for The companies CTO.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28294bea",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae13f5f",
   "metadata": {},
   "source": [
    "1.\tCamp, D. (2023). Pandas Read csv() Tutorial: Importing Data. [online] https://www.datacamp.com/tutorial/pandas-read-csv. Available at: https://www.datacamp.com/tutorial/pandas-read-csv.\n",
    "\n",
    "2.\tGeeksforgeeks (2018a). Get unique values from a column in Pandas DataFrame. [online] GeeksforGeeks. Available at: https://www.geeksforgeeks.org/get-unique-values-from-a-column-in-pandas-dataframe/ [Accessed 10 Dec. 2018].\n",
    "\n",
    "3.\tGeeksforgeeks (2018b). Pandas isnull() and notnull() Method. [online] GeeksforGeeks. Available at: https://www.geeksforgeeks.org/python-pandas-isnull-and-notnull/?ref=gcse [Accessed 1 Nov. 2023].\n",
    "\n",
    "4.\tIdris, I. (2011). NumPy 1.5 : beginner’s guide. Birmingham: Packt Publishing.\n",
    "\n",
    "5.\tJohn Paul Mueller and Luca Massaron (2023). Python for Data Science For Dummies. John Wiley & Sons.\n",
    "\n",
    "6.\tNik (2021). Pandas: Number of Rows in a Dataframe (6 Ways) • datagy. [online] datagy. Available at: https://datagy.io/pandas-number-of-rows/.\n",
    "\n",
    "7.\tPandas (2023a). Chart Visualization — pandas 1.3.3 documentation. [online] pandas.pydata.org. Available at: https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html.\n",
    "\n",
    "8.\tPandas (2023b). pandas.DataFrame.describe — pandas 1.0.3 documentation. [online] pandas.pydata.org. Available at: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html.\n",
    "\n",
    "9.\tPandas (2023c). Working with missing data — pandas 1.2.4 documentation. [online] pandas.pydata.org. Available at: https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html.\n",
    "\n",
    "10.\tPratusevich, M. (2017a). Practice Python. [online] Practicepython.org. Available at: https://www.practicepython.org/.\n",
    "\n",
    "11.\tPratusevich, M. (2017b). Practice Python. [online] Practicepython.org. Available at: https://www.practicepython.org/.\n",
    "\n",
    "12.\tSaravanan (2021). Import Pandas as pd. [online] Medium. Available at: https://medium.com/analytics-vidhya/import-pandas-as-pd-26344cd7235e.\n",
    "\n",
    "13.\tScikit Learn (2009). sklearn.decomposition.PCA — scikit-learn 0.20.3 Documentation. [online] Scikit-learn.org. Available at: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html [Accessed 2023].\n",
    "\n",
    "14.\tSeaborn (2023). seaborn.boxplot — seaborn 0.11.1 documentation. [online] seaborn.pydata.org. Available at: https://seaborn.pydata.org/generated/seaborn.boxplot.html.\n",
    "\n",
    "15.\tStack Overflow (2014a). Is there a way to determine the order of labels in scikit-learn’s LabelEncoder? [online] Stack Overflow. Available at: https://stackoverflow.com/questions/25343411/is-there-a-way-to-determine-the-order-of-labels-in-scikit-learns-labelencoder [Accessed 1 Nov. 2023].\n",
    "\n",
    "16.\tStack Overflow (2014b). Python - Principal Components Analysis Using Pandas Dataframe. [online] Stack Overflow. Available at: https://stackoverflow.com/questions/23282130/principal-components-analysis-using-pandas-dataframe.\n",
    "\n",
    "17.\tStack Overflow (2018). understand df.isnull.mean() in python. [online] Stack Overflow. Available at: https://stackoverflow.com/questions/53283142/understand-df-isnull-mean-in-python.\n",
    "\n",
    "18.\tStack Overflow (2019). How to run Python Code with ‘%matplotlib inline’? [online] Stack Overflow. Available at: https://stackoverflow.com/questions/55687832/how-to-run-python-code-with-matplotlib-inline.\n",
    "\n",
    "19.\tStack Overflow (2021). How to Generate Random Colors in matplotlib? [online] Stack Overflow. Available at: https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib [Accessed 1 Nov. 2023].\n",
    "\n",
    "20.\tStack overflow (2015). Python scikit learn pca.explained_variance_ratio_ cutoff. [online] Stack Overflow. Available at: https://stackoverflow.com/questions/32857029/python-scikit-learn-pca-explained-variance-ratio-cutoff.\n",
    "21.\tStack overflow (2018). Understanding inplace=True in pandas. [online] Stack Overflow. Available at: https://stackoverflow.com/questions/43893457/understanding-inplace-true-in-pandas [Accessed 1 Nov. 2023].\n",
    "\n",
    "22.\tStackExchange (2019). What Does the PCA().transform() Method do? [online] Cross Validated. Available at: https://stats.stackexchange.com/questions/409176/what-does-the-pca-transform-method-do [Accessed 1 Nov. 2023].\n",
    "\n",
    "23.\tStackOverflow (2012). How to run Python Code with ‘%matplotlib inline’? [online] Stack Overflow. Available at: https://stackoverflow.com/questions/55687832/how-to-run-python-code-with-matplotlib-inline [Accessed 1 Nov. 2023].\n",
    "\n",
    "24.\tStackoverflow (2011). r - Subset of rows containing NA (missing) values in a chosen column of a data frame. [online] Stack Overflow. Available at: https://stackoverflow.com/questions/7980622/subset-of-rows-containing-na-missing-values-in-a-chosen-column-of-a-data-frame.\n",
    "\n",
    "25.\tStackoverflow (2016). How to resolve AttributeError: ‘DataFrame’ object has no attribute. [online] Stack Overflow. Available at: https://stackoverflow.com/questions/38134643/how-to-resolve-attributeerror-dataframe-object-has-no-attribute [Accessed 1 Nov. 2023].\n",
    "\n",
    "26.\tStackoverflow (2021). Python - isnull().sum() vs isnull().count(). [online] Stack Overflow. Available at: https://stackoverflow.com/questions/60249807/python-isnull-sum-vs-isnull-count [Accessed 1 Nov. 2023].\n",
    "\n",
    "27.\tVishal (2019). Python Basic Exercise for Beginners. [online] PYnative. Available at: https://pynative.com/python-basic-exercise-for-beginners/.\n",
    "\n",
    "28.\tw3school (2023). Pandas DataFrame head() Method. [online] www.w3schools.com. Available at: https://www.w3schools.com/python/pandas/ref_df_head.asp.\n",
    "\n",
    "29. Python (2023). Warnings — Warning Control — Python 3.8.5 Documentation. [online] docs.python.org. Available at: https://docs.python.org/3/library/warnings.html.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f298217",
   "metadata": {},
   "source": [
    "# Graphs : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff263bd",
   "metadata": {},
   "source": [
    "Graph 1: Full Datasets Boxplot\n",
    "\n",
    "Graph 2:sns.boxplot(x=aps_failure_df['ca_000'])\n",
    "\n",
    "Graph 3:sns.boxplot(x=aps_failure_df['da_000'])\n",
    "\n",
    "Graph 4:sns.boxplot(x=aps_failure_df['ee_000'])\n",
    "\n",
    "Graph 5:sns.boxplot(x=aps_failure_df['aa_000'])\n",
    "\n",
    "Graph 6: plt.scatter-Random Range\n",
    "\n",
    "Graph 7: Explained Variance Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1140e227",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
